<html>

<head>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:wght@100;300;400&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link href="https://fonts.googleapis.com/css2?family=Inter+Tight:wght@500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="static/css/index.css">
  <script type="text/x-mathjax-config">
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ["\(", "\)"]],
        processEscapes: true,
      }
    }
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <style>
    :root {
      --edge: 128px;
      --edge_three: 2px;
      --width: 410px;
      --pagewidth: 1078px;
      --width3: 357px;
      --width4: 267px;
      --width5: 213px;
      --zwidth: 164.0px;
      --citation-color: linear-gradient(rgba(240, 230, 255, 1), rgba(240, 230, 255, 0.65));
      --citation-border-color: #d1b3ff;
      --box-color: #dfe9ec;
      --box-background-color: #9da8ab;
      --hover-color: #077fa4;
	    --hover-background: rgba(223, 233, 236, 1);
      --citation-color: #e1e1ac;
	    --citation-background: #f9f9e8;
      --accent-color: #0f5970;
    }

    body {
      font-family: IBM Plex Sans, sans-serif;
      padding: 20px 0;
      font-size: 13.5pt;
      font-weight: 100;
      line-height: 1.3em;
      hyphens: auto;
      -ms-hyphens: auto;
      -webkit-hyphens: auto;
      margin: auto;
      width: var(--pagewidth);
      position: relative;
      /* background-color:  #fbfefa; */
    }

    b {
      font-weight: 300;
    }

    /* h1 {
      font-variant: small-caps;
  } */

    h1, h2 {
      font-family: Inter Tight, cursive;
    }
    
    h1,
    h2,
    h3 {
      font-family: Inter Tight, cursive;
      font-weight: 400;
      text-align: center;
    }

    .section-title,
    details>summary {
      padding: 0.3em 0.5em;
      font-weight: 400;
      text-align: left;
      font-size: 1.3em;
      background-color: var(--box-color);
      /* list-style: none; */
      border: 0.5px solid var(--box-background-color);
      border-radius: 5px;
      margin: 4px 0;
    }

    .warning {
      padding: 0.5em;
      background-color: #ffffe4;
      /* list-style: none; */
      border: 0.5px solid #f7edae;
      border-radius: 5px;
      margin: 8px 2px;
    }

    .section-content {
      padding-bottom: 1em;
    }

    p {
      text-align: justify;
    }

    a {
      color: var(--accent-color);
      text-decoration: none;
    }

    a:hover {
      text-decoration: none;
      background-color: var(--hover-background); 
      color: var(--hover-color);
    }

    title {
      font-size: 28px;
      font-weight: bold;
    }

    /* Style the buttons that are used to open and close the accordion panel */
    .accordion {
      background-color: #eee;
      color: #444;
      cursor: pointer;
      padding: 8px;
      margin-top: 20px;
      margin-bottom: 2px;
      width: 100%;
      text-align: left;
      border: none;
      outline: none;
      transition: 0.4s;
      border-radius: 5px 5px 0px 0px;
    }

    .accordion:active,
    .accordion:focus {
      border-radius: 5px;
    }

    /* Add a background color to the button if it is clicked on (add the .active class with JS), and when you move the mouse over it (hover) */
    .active,
    .accordion:hover {
      background-color: #ccc;
    }

    .accordion:after {
      content: '\002B';
      color: #777;
      font-weight: bold;
      float: right;
      margin-left: 5px;
    }

    .active:after {
      content: "\2212";
    }
    
    .citation {
      font-family: monospace;
      padding: 0.5em 1em;
      font-size: 0.85em;
      line-height: 1em;
      text-align: left;
      border-radius: 5px;
      background: var(--citation-background);
      border: 1px solid var(--citation-color);
      box-shadow: 1px 1px 3px #ddd;
    }

    .paper {
      background: #fff;
      position: relative;
    }

    .paper img {
      max-height: 320px;
      z-index: 3;
      position: relative;
    }

    .paper, .paper::before, .paper::after {
      /* Styles to distinguish sheets from one another */
      box-shadow: 1px 1px 1px rgba(0,0,0,0.25);
      border: 1px solid #bbb;
    }

    .paper::before, .paper::after {
      content: "";
      position: absolute;
      height: 100%;
      width: 100%;
      background-color: #eee;
    }

    .paper::before {
      right: 6px;
      top: 1px;
      transform: rotate(-1.2deg);
      z-index: 2;
    }

    .paper::after {
      top: 3px;
      right: -3px;
      transform: rotate(1deg);
      z-index: 1;
    }

    .panel {
      display: block;
      position: relative;
      border: 1px solid #eee;
      border-radius: 0 0 5px 5px;
      padding-bottom: 1em;
      background-color: #fff;
    }

    /* Style the accordion panel. Note: hidden by default */
    .panel video {
      /* border: 1px solid #d4d4d4; */
      margin: 0px;
      padding: 0px;
    }

    .btn, .btn:active, .btn:focus {
      display: inline-block;
      color: #eee !important;
      border-color: var(--accent-color);
      background-color: var(--accent-color);
      font-size: 1.2em;
      border: 1px solid transparent;
      border-radius: 4px;
      padding: 10px 24px;
      z-index: 0;
    }

    .btn:hover {
      color: var(--hover-color) !important;
    }

    button {
      font-size: 16px;
      font-family: monospace;
      font-weight: bold;
    }

    .wrapper-box {
      display: block;
    }

    .caption {
      padding: 0.1em 0;
      margin: 0;
      font-size: 0.8em;
      font-weight: 400;
      text-align: center;
    }

    .box {
      display: flex;
      align-items: center;
      margin: 0;
      float: left;
      clear: right;
    }

    .bottom-left {
      position: absolute;
      bottom: 8px;
      left: 8px;
      font-size: 20px;
      font-weight: 400;
      color: #fff;
      text-shadow: 1px 1px 2px #444;
    }

    .top-left {
      position: absolute;
      top: 8px;
      left: 8px;
      font-size: 20px;
      font-weight: 400;
      color: #fff;
      text-shadow: 1px 1px 2px #444;
    }

    .top-left-2 {
      position: absolute;
      left: calc(var(--width) + 8px);
      top: 8px;
      font-size: 20px;
      font-weight: 400;
      color: #fff;
      text-shadow: 1px 1px 2px #444;
    }

    button img {
      border-radius: 15%;
      width: 46px;
      margin-right: 10px;
    }


    .vid-zoom-lens {
      position: absolute;
      border: 2px solid #FBB040;
      /*set the size of the lens:*/
      /* width: var(--zwidth);
      height: var(--zwidth); */
      border-radius: 3%;
      box-shadow: 1px 1px 6px 1px rgba(0, 0, 0, 0.2);
    }

    .two-block {
      width: var(--width);
      height: var(--width);
    }

    .three-block {
      width: var(--width3);
      height: var(--width3);
    }

    .four-block {
      width: var(--width4);
      height: var(--width4);
    }

    .five-block {
      width: var(--width5);
      height: var(--width5);
    }

    .vid-box {
      outline: 1px solid #eee;
      overflow: hidden;
      display: block;
    }

    .vid-container {
      float: left;
      display: block;
    }

    #video-compare-container {
      display: inline-block;
      line-height: 0;
      position: relative;
      width: 100%;
      padding-top: 100%;
    }

    #video-compare-container>video {
      width: 100%;
      position: absolute;
      top: 0;
      height: 100%;
    }

    #video-clipper {
      width: 50%;
      position: absolute;
      top: 0;
      bottom: 0;
      overflow: hidden;
    }

    #video-clipper video {
      width: 200%;
      position: absolute;
      height: 100%;
    }

    table {
      border-collapse: collapse;
      width: 100%;
      font-size: 90%;
      font-weight: 100;
    }

    th {
      background-color: #eee;
    }

    td,
    th {
      border: 1px solid #dddddd;
      text-align: left;
      padding: 8px;
    }

    .ours {
      background-color: #deecf0fe;
    }

    /* tr:nth-child(even) {
      background-color: #eee;
    } */
  </style>
</head>

<body>
  <div class="panel" style="display: block; border: none; height: 370px; padding: 5px var(--edge)">
    <h1><span style="font-variant: small-caps;">AnytimeCL</span>:</h1>
    <h2>Anytime Continual Learning for Open Vocabulary Classification</h2>
    <h3>ECCV 2024 <b>Oral presentation</b> | <a href="https://github.com/jessemelpolio/AnytimeCL">Codebase</a></h3>
    <h3>
      <a href="https://zzhu.vision" target="_blank">Zhen Zhu</a>, 
      <a href="https://github.com/nickgong1" target="_blank">Yiming Gong</a>, 
      <a href="http://dhoiem.cs.illinois.edu" target="_blank">Derek Hoiem</a>
    </h3>
    <p style="text-align: center;">University of Illinois at Urbana Champaign</p>
    <!-- <p class="warning" style="text-align: center; font-size: 90%">&nbsp;This page contains a large number of videos. -->
      <!-- If they don't play or go out of sync, please reload.<br>We recommend to use Chrome as your browser.</p> -->
  </div>
  <div class="panel" style="display: block; border: none; height: 580px; padding: 5px var(--edge)">
    <div class="vid-container">
      <div class="vid-box">
        <video class="chestnut" width="820" height="410" autoplay controls loop muted>
          <source src="resources/Teaser/chestnut_comp.mp4" type="video/mp4">
        </video>
      </div>
      <p>We present <span style="font-variant: small-caps;">SuperGaussian</span>, a novel method that repurposes
        existing video upsampling methods for the 3D superresolution task. <span
          style="font-variant: small-caps;">SuperGaussian</span> can handle various input types such as NeRFs, Gaussian
        Splats, reconstructions obtained from noisy scans, models that are generated by recent text-to-3D methods, or
        low-poly meshes. <span style="font-variant: small-caps;">SuperGaussian</span> generates high resolution 3D
        outputs with rich geometric and texture details in the form of Gaussian Splats.</p>
    </div>
  </div>
  <h2 class="section-title">Abstract</h2>
    <p>We propose an approach for anytime continual learning (AnytimeCL) for open vocabulary image classification. The AnytimeCL problem aims to break away from batch training and rigid models by requiring that a system can predict any set of labels at any time and efficiently update and improve when receiving one or more training samples at any time. Despite the challenging goal, we achieve substantial improvements over recent methods. We propose a dynamic weighting between predictions of a partially fine-tuned model and a fixed open vocabulary model that enables continual improvement when training samples are available for a subset of a task's labels. We also propose an attention-weighted PCA compression of training features that reduces storage and computation with little impact to model accuracy. Our methods are validated with experiments that test flexibility of learning and inference.</p>

  <h2 class="section-title">Video Overview</h2>
  <div class="panel" style="display: block; border: none; height: 582px; padding: 5px calc((var(--pagewidth) - 864px)/2)">
    <div class="vid-box">
      <video class="chestnut" width="864" height="576" controls>
        <source src="supplementary_video.mp4" type="video/mp4">
      </video>
    </div>
  </div>

  <h2 class="section-title">Pipeline in a Nutshell</h2>
  <div class="panel" style="padding: 5px 4px; height: 380px; border: none;">
    <p> Given an input low-res 3D representation, which can be in various formats, we first sample a smooth camera
      trajectory and render an intermediate low-resolution video. We first upsample this video using existing video
      upsamplers and obtain a higher resolution 3D representation that has sharper and more vivid details. Next, we
      perform 3D optimization to improve geometric and texture details. Our method, <b><span
          style="font-variant: small-caps;">SuperGaussian</span></b>, produces a final 3D representation in the form of
      high-resolution Gaussian Splats.</p>

    <div class="vid-container">
      <div class="vid-box five-block">
        <video width="213" height="213" autoplay controls loop muted>
          <source src="resources/SuperGaussian/dinosaur_low_res.mp4" type="video/mp4">
        </video>
      </div>
      <p class="caption">Low-res input</p>
    </div>
    <div class="vid-container">
      <div class="vid-box" style="outline: none; height='213px'">
        <video width="267" autoplay controls loop muted>
          <source src="resources/Pipeline/trajectories.mp4" type="video/mp4">
        </video>
      </div>
      <p class="caption">Trajectory sampling</p>
    </div>
    <div class="vid-container">
      <div class="vid-box" style="outline: none;">
        <img width="375" src="resources/Pipeline/pipeline.png">
      </div>
      <p class="caption">Video Upsampling</p>
    </div>
    <div class="vid-container">
      <div class="vid-box five-block">
        <video width="213" height="213" autoplay controls loop muted>
          <source src="resources/SuperGaussian/dinosaur_ours_with_fitting_iter_3.mp4" type="video/mp4">
        </video>
      </div>
      <p class="caption">Reconstruction</p>
    </div>
  </div>
  <h2 class="section-title">Paper</h2>
  <div class="panel" style="display: flex; align-items: center; justify-content: center; height: 420px; border: none;">
    <a class="btn btn-primary" href="https://arxiv.org/abs/2406.00609" target="_blank">
      <span>
        <div class="paper"><img src="resources/Thumbnails/paper_screenshot.png"/></div>
        <br>
        <b style="display: flex; align-items: center; justify-content: center; padding: 6px 12px;"><i class="ai ai-arxiv ai-1x"></i> arXiv Page</b>
      </span>
    </a>
  </div>
  <div class="panel" style="height: 120px; border: none;">
    <p class="citation">@inproceedings{zhu2024anytimecl,<br>
      &nbsp;&nbsp;title = {Anytime Continual Learning for Open Vocabulary Classification},<br>
      &nbsp;&nbsp;author = {Zhu, Zhen and Gong, Yiming and Hoiem, Derek},<br>
      &nbsp;&nbsp;booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},<br>
      &nbsp;&nbsp;year = {2024},<br>
     }
     </p>
  </div>
  <details open>
    <summary>Online Class-wise Weighting (OCW)</summary>
      <p>
        We keep two model branches, one is continuasly tuned, and another one is fixed. While both branches will provide a prediction, we would like to assign more weight to the model that is more likely to be correct for a given label. 
        c<sub>t</sub>(y) and c<sub>o</sub>(y) are the estimated accuracy for tuned and fix model for label y. &epsilon; is a very small number (1e-8) to prevent division by zero.
      </p>
      <img src="resources/ocw.png" alt="MY ALT TEXT" class="centered-image"/>
      <p>
        We use EMA to estimate the accuracy of samples ( and ) before its gradient step by the tuned and original model, for a given class.
      </p>
      <img src="resources/ema.png" alt="MY ALT TEXT" class="centered-image"/>
      <p>
        Here, \(\hat{\text{c}}\)<sub>t</sub> is the estimated accuracy of label y in the previous step; y<sub>t</sub>(x) denotes
        the predicted label of the tuned model for x. η is the EMA decay factor. Since the exponential moving
        average depends on past values, we compute ct(y) as the average accuracy for
        the first <sup>1</sup>/<sub>1-η</sub> samples. c<sub>o</sub>(y) is updated in the same way.
      </p>

  </details>
  <details open>
    <summary> Online training strategy </summary>
    <p>A training strategy that converts batch training to 1 new 
      sample with the rest from old data per batch with class-
      balanced sampling.
    </p>
    <img src="resources/online_training.png" alt="MY ALT TEXT"  class="centered-image"/>
  </details>
  <details>
    <summary>Task Incremental</summary>
    <p>Task incremental setting is to sequentially train over target tasks and evaluate the average accuracy on all tasks.</p>
    <img src="resources/task_incremental.png" alt="MY ALT TEXT"  class="centered-image"/>
    <p>
      <strong>Task Incremental Results:</strong>
    </p>
    <img src="resources/task_incremental_result.png" alt="MY ALT TEXT"  class="centered-image"/>
  </details>
  <details>
    <summary>Class Incremental</summary>
    <p>
      Class incremental setting sequentially train over a subset of classes of each target task.
    </p>
    <img src="resources/class_incremental.png" alt="MY ALT TEXT"  class="centered-image"/>
    <p>
      <strong>Class Incremental Results:</strong>
    </p>
    <img src="resources/class_incremental_result.png" alt="MY ALT TEXT"  class="centered-image"/>
  </details>
  <details>
    <summary>Data Incremental</summary>
    <p>
      Data incremental setting sequentially train over a subset of samples of each target task.
    </p>
    <img src="resources/data_incremental.png" alt="MY ALT TEXT"  class="centered-image"/>
    <p>
      <strong>Class Incremental Results:</strong>
    </p>
    <img src="resources/data_incremental_result.png" alt="MY ALT TEXT"  class="centered-image"/>
  </details>
  <details>
    <summary>Combination with other vision models</summary>
    <p>
      Our method generalize beyond the original CLIP model as we observe that replacing the tuned
      model with DINOv2 results in consistent performance improvements at every
      stage, with a notably steeper improvement curve in later stages.
    </p>
    <img src="resources/other_model.png" alt="MY ALT TEXT"  class="centered-image"/>
  </details>
  <details>
    <summary>Abalation</summary>
    <p>
      <strong>Weighting Strategies</strong>: Weighting is vital for our dual decoder approach. We
      compare several ways to compute the weights: 1) CLIP: namely α<sub>t</sub>(y) = 0 for any images; 2) Tuned model only: α<sub>t</sub>(y) = 1 for any images; 3) AIM; 4)
      OCW (0/1): a variant of OCW where we round α<sub>t</sub>(y) to 0 or 1 to use either
      the original or tuned model; 5) Our proposed OCW. We partially finetune the
      decoder with fixed label embeddings and combine the tuned model with the orig-
      inal model using different weighting strategies.
    </p>
    <img src="resources/weighting_ablat.png" alt="MY ALT TEXT"  class="centered-image"/>
    
    <p>
      <strong>Tuned Parts</strong>: Our proposed method tunes the last transformer block while
      keeping the label encoder fixed. In Fig. 4 (d), we compare this approach to al-
      ternatives of tuning only the label encoder, both the block and the label encoder,
      or neither of them, under the flexible inference test. When only using the tuned
      model for comparison (α<sub>t</sub> = 1), fine tuning only the last transformer best retains
      predictive ability for novel labels.
    </p>
    <img src="resources/tune_ablat.png" alt="MY ALT TEXT"  class="centered-image"/>

    <p>
      <strong>Sampling Methods</strong>: We compare different methods for sampling from the
      stored samples. FIFO cycles through samples in order of first appearance, “uni-
      form” randomly draws samples for each batch, class-balanced (which we use in
      all other experiments) samples classes, and frequency weighted sampling (FWS)
      samples based on how many times a given sample has been batched in training.
      Class-balanced and uniform are similar in practice, and perform best.
    </p>
    <img src="resources/sampling_ablat.png" alt="MY ALT TEXT"  class="centered-image"/>

    <p>
      <strong>The "Other" Logit Regularization</strong>: We assess the impact of “other” logit regularization in the union data incremental
      scenario. The results demonstrate consistent enhancements when this regular-
      ization is applied, compared to its absence.
    </p>
    <img src="resources/other_ablat.png" alt="MY ALT TEXT"  class="centered-image"/>
  </details>
  <details>
    <summary>Data Compression</summary>
    <p>
      We perform PCA based data compression and propose our own attention-weighted PCA, which saves 30x the storage while achieving nearly the same accuracy compared to processing the full image or full features.
    </p>
    <img src="resources/compression.png" alt="MY ALT TEXT"  class="centered-image"/>
  </details>
  <script>
    var acc = document.getElementsByClassName('accordion');
    var i;
    for (i = 0; i < acc.length; i++) {
      acc[i].addEventListener('click', function () {
        this.classList.toggle('active');
        var panel = this.nextElementSibling;
        if (panel.style.display === 'block') {
          panel.style.display = 'none';
        } else {
          panel.style.display = 'block';
        }
      });
    }

    function playSync(vidID) {
      resultClass = vidID + "_group";
      var videos = document.getElementsByClassName(resultClass);

      for (var i = 0; i < videos.length; i++) {
        v = videos.item(i)

        rest = []
        for (var j = 0; j < videos.length; j++) {
          if (j != i) {
            rest.push(videos.item(j))
          }
        }
        v.addEventListener(
          'play',
          function () {
            v.play();
            for (var j = 0; j < rest.length; j++) {
              rest[j].play();
            }
          },
          false);

        v.onclick = function () {
          if (v.paused) {
            v.play();
            for (var j = 0; j < rest.length; j++) {
              rest[j].play();
            }
          } else {
            v.pause();
            for (var j = 0; j < rest.length; j++) {
              rest[j].pause();
            }
          }

          return false;
        };
      }
    }


    function videoZoom(vidID, factor = 2, edgeLeft = 0, edgeTop = 0) {
      var vid, lens, result, cx, cy;
      vid = document.getElementById(vidID);
      resultClass = vidID + "_zoom";
      result = document.getElementsByClassName(resultClass);
      /* Create lens: */
      lens = document.createElement("DIV");
      lens.setAttribute("class", "vid-zoom-lens");
      /* Insert lens: */
      vid.parentElement.insertBefore(lens, vid);

      sz = vid.width;
      initPos = (sz - (sz / factor)) / 2;
      /* Init lens and zoom video at center: */
      x = y = initPos
      console.log("setting up lens for " + vidID + " at " + x + ", " + y + ", size " + sz);
      lens.style.width = (sz / factor) + "px";
      lens.style.height = (sz / factor) + "px";
      lens.style.left = x + "px";
      lens.style.top = y + "px";
      lens.style.marginLeft = edgeLeft + "px";
      lens.style.marginTop = edgeTop + "px";

      for (var i = 0; i < result.length; i++) {
        /* Display what the lens "sees": */
        result.item(i).style.marginLeft = "-" + (x * factor) + "px"
        result.item(i).style.marginTop = "-" + (y * factor) + "px";
      }
      /* Calculate the ratio between result DIV and lens: */

      /* Set background properties for the result DIV */
      // result.style.backgroundImage = "url('" + img.src + "')";
      // result.style.backgroundSize = (img.width * cx) + "px " + (img.height * cy) + "px";
      /* Execute a function when someone moves the cursor over the image, or the lens: */
      lens.addEventListener("mousemove", moveLens);
      vid.addEventListener("mousemove", moveLens);
      /* And also for touch screens: */
      lens.addEventListener("touchmove", moveLens);
      vid.addEventListener("touchmove", moveLens);
      function moveLens(e) {
        var pos, x, y;
        /* Prevent any other actions that may occur when moving over the image */
        e.preventDefault();
        /* Get the cursor's x and y positions: */
        pos = getCursorPos(e);
        /* Calculate the position of the lens: */
        x = pos.x - (lens.offsetWidth / 2);
        y = pos.y - (lens.offsetHeight / 2);
        /* Prevent the lens from being positioned outside the image: */
        min_x = vid.width - lens.offsetWidth
        if (x > min_x) { x = min_x; }
        if (x < 0) { x = 0; }
        if (y > vid.height - lens.offsetHeight) { y = vid.height - lens.offsetHeight; }
        if (y < 0) { y = 0; }
        /* Set the position of the lens: */
        lens.style.left = x + "px";
        lens.style.top = y + "px";
        for (var i = 0; i < result.length; i++) {
          /* Display what the lens "sees": */
          result.item(i).style.marginLeft = "-" + (x * factor) + "px"
          result.item(i).style.marginTop = "-" + (y * factor) + "px";
        }

      }
      function getCursorPos(e) {
        var a, x = 0, y = 0;
        e = e || window.event;
        /* Get the x and y positions of the image: */
        a = vid.getBoundingClientRect();
        /* Calculate the cursor's x and y coordinates, relative to the image: */
        x = e.pageX - a.left;
        y = e.pageY - a.top;
        /* Consider any page scrolling: */
        x = x - window.pageXOffset;
        y = y - window.pageYOffset;
        return { x: x, y: y };
      }
    }

    // Fetch all the details element.
    const details = document.querySelectorAll("details");

    // Add the onclick listeners.
    details.forEach((detail) => {
      detail.addEventListener("toggle", () => {
        if (detail.open) setTargetDetail(detail);
      });
    });

    // Close all the details that are not targetDetail.
    function setTargetDetail(targetDetail) {
      details.forEach((detail) => {
        if (detail !== targetDetail) {
          detail.open = false;
        } else {
          detail.open = true;
          detail.scrollIntoView();
        }
      });
    }

    // function trackLocation(e) {
    //   var rect = videoContainer.getBoundingClientRect(),
    //       position = ((e.pageX - rect.left) / videoContainer.offsetWidth)*100;
    //   if (position <= 100) { 
    //     videoClipper.style.width = position+"%";
    //     clippedVideo.style.width = ((100/position)*100)+"%";
    //     clippedVideo.style.zIndex = 3;
    //  }
    // }
    // var videoContainer = document.getElementById("video-compare-container"),
    // videoClipper = document.getElementById("video-clipper"),
    // clippedVideo = videoClipper.getElementsByTagName("video")[0];
    // videoContainer.addEventListener( "mousemove", trackLocation, false); 
    // videoContainer.addEventListener("touchstart",trackLocation,false);
    // videoContainer.addEventListener("touchmove",trackLocation,false);

    videoZoom("backpack", 2.5, 128, 5)
    videoZoom("unicorn backpack", 2.5, 128, 5)
    videoZoom("flowers_1", 2.5, 128, 5)
    videoZoom("flowers_2", 2.5, 128, 5)
    videoZoom("shoes_1", 2.5, 128, 5)
    videoZoom("striped_slippers", 2.5, 128, 5)
    videoZoom("shoes_2", 2.5, 128, 5)
    videoZoom("pineapple", 2.5, 128, 5)
    videoZoom("sports_car", 2.5, 128, 5)
    videoZoom("truck", 2.5, 128, 5)
    videoZoom("wicker_bowl", 2.5, 128, 5)
    videoZoom("basket", 2.5, 128, 5)
    videoZoom("train", 2.5, 128, 5)
    videoZoom("toy_plane", 2.5, 128, 5)
    videoZoom("teddy_bear", 2.5, 128, 5)
    videoZoom("plush_hedgehog", 2.5, 128, 5)
    videoZoom("dragonfly", 2.5, 128, 5)
    videoZoom("toy_horse", 2.5, 128, 5)
    videoZoom("cute_tiger", 2.5, 128, 5)
    videoZoom("donut", 2.5, 128, 5)
    videoZoom("chair", 2.5, 128, 5)
    videoZoom("drums", 2.5, 128, 5)
    videoZoom("ficus", 2.5, 128, 5)
    videoZoom("hotdog", 2.5, 128, 5)
    videoZoom("mic", 2.5, 128, 5)
    videoZoom("ship", 2.5, 128, 5)
    videoZoom("dragon", 2.5, 128, 5)
    videoZoom("frog", 2.5, 128, 5)
    videoZoom("mushroom", 2.5, 128, 5)
    videoZoom("handbag_BS", 3, 5, 5)
    videoZoom("tiger_BS", 3, 5, 5)
    videoZoom("lion_slipper_BS", 3, 5, 5)
    videoZoom("broccoli_BS", 3, 5, 5)
    videoZoom("insect", 2.5, 2, 5)
  </script>
</body>

</html>
